"""
Integrate-and-Fire Neural Network with White-Box, Black-Box, and Hybrid Constraints
====================================================================================

This enhanced implementation demonstrates:
1. WHITE-BOX CONSTRAINTS: Hard, interpretable biological rules (invariants, OOD robustness)
2. BLACK-BOX CONSTRAINTS: Soft, DNN-learned constraints (data-driven, complex patterns)
3. HYBRID CONSTRAINTS: Combining both approaches for optimal performance

Key Concepts:
-------------
WHITE-BOX (Hard Constraints):
- Pros: Interpretable, guaranteed biological plausibility, OOD generalization
- Cons: Requires expert knowledge, may miss complex patterns
- Examples: Dale's law, firing rate limits, membrane potential bounds

BLACK-BOX (Soft Constraints via DNN):
- Pros: Data-driven discovery, captures complex relationships, flexible
- Cons: Black-box nature, no guarantees, may violate biology
- Examples: Learned network topology rules, adaptive weight bounds

HYBRID (Best of Both):
- Combines hard biological invariants with learned soft constraints
- Enforces critical rules while allowing data-driven refinement
- Ideal for robust, interpretable, yet powerful models
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
from collections import defaultdict
from typing import List, Dict, Tuple, Optional
import time

# Set random seed for reproducibility
torch.manual_seed(42)
np.random.seed(42)


# ============================================================================
# 1. WHITE-BOX CONSTRAINTS (Hard Biological Rules)
# ============================================================================


class WhiteBoxConstraints:
    """
    White-box (hard) constraints based on known biological principles.
    
    Advantages:
    - Interpretable and explainable
    - Guaranteed to satisfy biological invariants
    - Robust to out-of-distribution (OOD) inputs
    - No training required
    
    Disadvantages:
    - Requires expert domain knowledge
    - May be too restrictive for complex phenomena
    - Cannot discover new patterns from data
    """
    
    def __init__(
        self,
        n_neurons: int,
        enforce_dales_law: bool = True,
        max_firing_rate: float = 100.0,  # Hz
        min_weight: float = -2.0,
        max_weight: float = 2.0,
        max_membrane_potential: float = 1.0,
        min_membrane_potential: float = -0.5,
    ):
        self.n_neurons = n_neurons
        self.enforce_dales_law = enforce_dales_law
        self.max_firing_rate = max_firing_rate
        self.min_weight = min_weight
        self.max_weight = max_weight
        self.max_membrane_potential = max_membrane_potential
        self.min_membrane_potential = min_membrane_potential
        
        # Dale's law: Each neuron is either excitatory or inhibitory (not both)
        # Randomly assign neuron types (80% excitatory, 20% inhibitory - biologically realistic)
        self.neuron_types = torch.rand(n_neurons) > 0.2  # True = excitatory, False = inhibitory
        
    def apply_weight_constraints(self, weights: torch.Tensor) -> torch.Tensor:
        """
        Apply hard constraints to connection weights.
        
        Constraints enforced:
        1. Dale's law: neurons can only have excitatory OR inhibitory outputs
        2. Weight magnitude bounds
        3. No self-connections
        """
        constrained_weights = weights.clone()
        
        # 1. Enforce Dale's law
        if self.enforce_dales_law:
            for i in range(self.n_neurons):
                if self.neuron_types[i]:  # Excitatory neuron
                    # All outgoing weights must be positive
                    constrained_weights[i, :] = torch.clamp(constrained_weights[i, :], min=0.0)
                else:  # Inhibitory neuron
                    # All outgoing weights must be negative
                    constrained_weights[i, :] = torch.clamp(constrained_weights[i, :], max=0.0)
        
        # 2. Enforce weight bounds
        constrained_weights = torch.clamp(
            constrained_weights, 
            min=self.min_weight, 
            max=self.max_weight
        )
        
        # 3. No self-connections
        constrained_weights.fill_diagonal_(0.0)
        
        return constrained_weights
    
    def apply_membrane_potential_constraint(self, voltage: float) -> float:
        """Enforce membrane potential bounds (prevents numerical instability)."""
        return np.clip(voltage, self.min_membrane_potential, self.max_membrane_potential)
    
    def check_firing_rate_constraint(
        self, 
        spike_count: int, 
        time_window: float
    ) -> bool:
        """
        Check if firing rate is within biological limits.
        Returns True if constraint is satisfied.
        """
        firing_rate = spike_count / (time_window / 1000.0)  # Convert to Hz
        return firing_rate <= self.max_firing_rate
    
    def get_violation_report(self, weights: torch.Tensor, verbose: bool = False) -> Dict:
        """Generate a report of constraint violations."""
        violations = {
            'dales_law_violations': 0,
            'weight_bound_violations': 0,
            'self_connection_violations': 0,
            'total_violations': 0
        }
        
        # Check Dale's law
        if self.enforce_dales_law:
            for i in range(self.n_neurons):
                if self.neuron_types[i]:  # Excitatory
                    violations['dales_law_violations'] += (weights[i, :] < 0).sum().item()
                else:  # Inhibitory
                    violations['dales_law_violations'] += (weights[i, :] > 0).sum().item()
        
        # Check weight bounds
        violations['weight_bound_violations'] = (
            (weights < self.min_weight) | (weights > self.max_weight)
        ).sum().item()
        
        # Check self-connections
        violations['self_connection_violations'] = (torch.diagonal(weights) != 0).sum().item()
        
        violations['total_violations'] = (
            violations['dales_law_violations'] + 
            violations['weight_bound_violations'] + 
            violations['self_connection_violations']
        )
        
        if verbose:
            print("\nüîç White-Box Constraint Violation Report:")
            print(f"  Dale's Law violations: {violations['dales_law_violations']}")
            print(f"  Weight bound violations: {violations['weight_bound_violations']}")
            print(f"  Self-connection violations: {violations['self_connection_violations']}")
            print(f"  Total violations: {violations['total_violations']}")
        
        return violations


# ============================================================================
# 2. BLACK-BOX CONSTRAINTS (Learned via DNN)
# ============================================================================


class BlackBoxConstraintNetwork(nn.Module):
    """
    Black-box (soft) constraints learned from data via DNN.
    
    Advantages:
    - Discovers complex patterns from data automatically
    - Flexible and adaptive
    - Can capture high-order interactions
    - No need for explicit biological knowledge
    
    Disadvantages:
    - Black-box (not interpretable)
    - No guarantees on biological plausibility
    - May fail on out-of-distribution inputs
    - Requires training data
    """
    
    def __init__(
        self, 
        n_neurons: int, 
        hidden_dim: int = 128,
        dropout_rate: float = 0.1
    ):
        super(BlackBoxConstraintNetwork, self).__init__()
        self.n_neurons = n_neurons
        
        # Neural network to learn constraint function
        # Input: proposed weight matrix (flattened)
        # Output: corrected weight matrix (flattened)
        self.constraint_net = nn.Sequential(
            nn.Linear(n_neurons * n_neurons, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.ReLU(),
            
            nn.Linear(hidden_dim // 2, n_neurons * n_neurons),
            nn.Tanh()  # Soft bounds on weights
        )
        
        # Learned weight scaling factors
        self.weight_scale = nn.Parameter(torch.ones(1) * 2.0)
        
    def forward(self, weights: torch.Tensor) -> torch.Tensor:
        """
        Apply learned soft constraints to weights.
        
        The network learns to:
        - Adjust weights based on network-level patterns
        - Enforce learned topology preferences
        - Balance excitation/inhibition (learned, not hard-coded)
        """
        batch_size = weights.shape[0] if weights.dim() == 3 else 1
        
        if weights.dim() == 2:
            weights = weights.unsqueeze(0)
        
        # Flatten weights for network processing
        flat_weights = weights.view(batch_size, -1)
        
        # Apply constraint network
        corrected_flat = self.constraint_net(flat_weights)
        
        # Scale and reshape
        corrected_weights = corrected_flat.view(batch_size, self.n_neurons, self.n_neurons)
        corrected_weights = corrected_weights * self.weight_scale
        
        return corrected_weights.squeeze(0) if batch_size == 1 else corrected_weights
    
    def train_from_data(
        self, 
        weight_samples: List[torch.Tensor],
        target_outputs: Optional[List[torch.Tensor]] = None,
        epochs: int = 100,
        lr: float = 0.001
    ):
        """
        Train the constraint network from example weight matrices.
        
        Args:
            weight_samples: List of valid weight matrices
            target_outputs: Optional target weight matrices (if None, uses samples as targets)
            epochs: Training epochs
            lr: Learning rate
        """
        optimizer = optim.Adam(self.parameters(), lr=lr)
        criterion = nn.MSELoss()
        
        if target_outputs is None:
            target_outputs = weight_samples
        
        print(f"\nüéì Training Black-Box Constraint Network ({epochs} epochs)...")
        
        for epoch in range(epochs):
            total_loss = 0.0
            
            for weight, target in zip(weight_samples, target_outputs):
                optimizer.zero_grad()
                
                # Forward pass
                predicted = self.forward(weight)
                
                # Loss: how well does the network preserve valid structure?
                loss = criterion(predicted, target)
                
                # Backward pass
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
            
            if (epoch + 1) % 20 == 0:
                avg_loss = total_loss / len(weight_samples)
                print(f"  Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.6f}")


# ============================================================================
# 3. HYBRID CONSTRAINTS (Best of Both Worlds)
# ============================================================================


class HybridConstraints:
    """
    Hybrid constraint system combining white-box and black-box approaches.
    
    Strategy:
    1. Apply white-box constraints first (hard biological rules)
    2. Apply black-box refinement (learned patterns)
    3. Verify white-box constraints still hold (critical invariants)
    
    Advantages:
    - Guaranteed biological plausibility (white-box)
    - Data-driven refinement (black-box)
    - Interpretable yet powerful
    - Robust to OOD scenarios
    
    This is the IDEAL approach for biological neural network modeling!
    """
    
    def __init__(
        self,
        n_neurons: int,
        white_box: WhiteBoxConstraints,
        black_box: BlackBoxConstraintNetwork,
        black_box_strength: float = 0.5  # How much to trust black-box (0-1)
    ):
        self.n_neurons = n_neurons
        self.white_box = white_box
        self.black_box = black_box
        self.black_box_strength = black_box_strength
        
    def apply_constraints(
        self, 
        weights: torch.Tensor,
        verbose: bool = False
    ) -> Tuple[torch.Tensor, Dict]:
        """
        Apply hybrid constraints in sequence.
        
        Process:
        1. White-box constraints (hard biological rules)
        2. Black-box refinement (learned patterns)
        3. Re-apply critical white-box constraints (ensure invariants)
        """
        
        # Step 1: Apply white-box constraints (hard rules)
        weights_wb = self.white_box.apply_weight_constraints(weights)
        
        # Step 2: Apply black-box refinement (soft learned patterns)
        with torch.no_grad():
            weights_bb = self.black_box(weights_wb)
        
        # Step 3: Blend white-box and black-box results
        # Use black_box_strength to control influence
        weights_blended = (
            (1 - self.black_box_strength) * weights_wb + 
            self.black_box_strength * weights_bb
        )
        
        # Step 4: Re-apply critical white-box constraints
        # This ensures biological invariants are ALWAYS satisfied
        weights_final = self.white_box.apply_weight_constraints(weights_blended)
        
        # Generate report
        report = {
            'original_violations': self.white_box.get_violation_report(weights),
            'after_white_box': self.white_box.get_violation_report(weights_wb),
            'after_black_box': self.white_box.get_violation_report(weights_bb),
            'final_violations': self.white_box.get_violation_report(weights_final),
            'white_box_change': torch.abs(weights_wb - weights).mean().item(),
            'black_box_change': torch.abs(weights_bb - weights_wb).mean().item(),
            'total_change': torch.abs(weights_final - weights).mean().item(),
        }
        
        if verbose:
            print("\nüîÄ Hybrid Constraint Application Report:")
            print(f"  Original ‚Üí White-box change: {report['white_box_change']:.6f}")
            print(f"  White-box ‚Üí Black-box change: {report['black_box_change']:.6f}")
            print(f"  Total change from original: {report['total_change']:.6f}")
            print(f"  Final violations: {report['final_violations']['total_violations']}")
        
        return weights_final, report


# ============================================================================
# 4. INTEGRATE-AND-FIRE NEURON WITH CONSTRAINT SUPPORT
# ============================================================================


class IntegrateAndFireNeuron:
    """Leaky integrate-and-fire neuron model with constraint awareness."""
    
    def __init__(
        self,
        neuron_id: int,
        tau_m: float = 5.0,
        v_threshold: float = 0.3,
        v_reset: float = 0.0,
        v_rest: float = 0.0,
        dt: float = 0.1,
        white_box_constraints: Optional[WhiteBoxConstraints] = None
    ):
        self.neuron_id = neuron_id
        self.tau_m = tau_m
        self.v_threshold = v_threshold
        self.v_reset = v_reset
        self.v_rest = v_rest
        self.dt = dt
        self.white_box = white_box_constraints
        
        self.v = v_rest
        self.spike_history = []
        self.input_history = []
        self.output_history = []
    
    def update(self, input_current: float) -> int:
        """Update neuron state with optional white-box constraint enforcement."""
        # Leaky integration
        dv = (-(self.v - self.v_rest) + input_current) / self.tau_m * self.dt
        self.v += dv
        
        # Apply membrane potential constraint if white-box constraints are provided
        if self.white_box is not None:
            self.v = self.white_box.apply_membrane_potential_constraint(self.v)
        
        # Check for spike
        spike = 0
        if self.v >= self.v_threshold:
            spike = 1
            self.v = self.v_reset
            self.spike_history.append(len(self.input_history))
        
        # Record I/O
        self.input_history.append(input_current)
        self.output_history.append(spike)
        
        return spike
    
    def reset(self):
        """Reset neuron to initial state."""
        self.v = self.v_rest
        self.spike_history = []
        self.input_history = []
        self.output_history = []


# ============================================================================
# 5. CONSTRAINED IF NETWORKS
# ============================================================================


class ConstrainedIFNetwork:
    """
    IF Network with selectable constraint type.
    
    Constraint Modes:
    - 'none': No constraints (baseline)
    - 'white_box': Hard biological constraints only
    - 'black_box': Learned soft constraints only
    - 'hybrid': Combined approach (recommended)
    """
    
    def __init__(
        self,
        n_neurons: int,
        connection_matrix: Optional[torch.Tensor] = None,
        constraint_mode: str = 'none',
        white_box_params: Optional[Dict] = None,
        black_box_params: Optional[Dict] = None,
        hybrid_params: Optional[Dict] = None
    ):
        self.n_neurons = n_neurons
        self.constraint_mode = constraint_mode
        
        # Initialize constraints based on mode
        self.white_box = None
        self.black_box = None
        self.hybrid = None
        
        if constraint_mode in ['white_box', 'hybrid']:
            wb_params = white_box_params or {}
            self.white_box = WhiteBoxConstraints(n_neurons, **wb_params)
        
        if constraint_mode in ['black_box', 'hybrid']:
            bb_params = black_box_params or {}
            self.black_box = BlackBoxConstraintNetwork(n_neurons, **bb_params)
        
        if constraint_mode == 'hybrid':
            h_params = hybrid_params or {}
            self.hybrid = HybridConstraints(
                n_neurons,
                self.white_box,
                self.black_box,
                **h_params
            )
        
        # Initialize neurons with white-box constraints if available
        self.neurons = [
            IntegrateAndFireNeuron(i, white_box_constraints=self.white_box)
            for i in range(n_neurons)
        ]
        
        # Initialize weights
        if connection_matrix is None:
            self.weights = torch.randn(n_neurons, n_neurons) * 0.5
            mask = torch.rand(n_neurons, n_neurons) > 0.8
            self.weights[mask] = 0.0
            self.weights.fill_diagonal_(0.0)
        else:
            self.weights = connection_matrix.clone()
        
        # Apply initial constraints
        self.apply_constraints()
        
        self.synaptic_delay = 1
    
    def apply_constraints(self, verbose: bool = False) -> Dict:
        """Apply constraints to network weights based on constraint mode."""
        report = {}
        
        if self.constraint_mode == 'none':
            report['type'] = 'none'
            report['changes'] = 0.0
            
        elif self.constraint_mode == 'white_box':
            original = self.weights.clone()
            self.weights = self.white_box.apply_weight_constraints(self.weights)
            report['type'] = 'white_box'
            report['changes'] = torch.abs(self.weights - original).mean().item()
            report['violations'] = self.white_box.get_violation_report(self.weights, verbose)
            
        elif self.constraint_mode == 'black_box':
            original = self.weights.clone()
            with torch.no_grad():
                self.weights = self.black_box(self.weights)
            report['type'] = 'black_box'
            report['changes'] = torch.abs(self.weights - original).mean().item()
            
        elif self.constraint_mode == 'hybrid':
            original = self.weights.clone()
            self.weights, hybrid_report = self.hybrid.apply_constraints(self.weights, verbose)
            report['type'] = 'hybrid'
            report['changes'] = torch.abs(self.weights - original).mean().item()
            report.update(hybrid_report)
        
        if verbose:
            print(f"\n‚öôÔ∏è  Applied '{self.constraint_mode}' constraints")
            print(f"   Average weight change: {report.get('changes', 0):.6f}")
        
        return report
    
    def step(self, external_inputs: torch.Tensor) -> torch.Tensor:
        """Run one simulation step."""
        spikes = torch.zeros(self.n_neurons)
        
        for i, neuron in enumerate(self.neurons):
            input_current = external_inputs[i].item()
            
            # Synaptic input
            for j in range(self.n_neurons):
                if self.weights[j, i] != 0:
                    if len(neuron.input_history) >= self.synaptic_delay:
                        idx = len(neuron.input_history) - self.synaptic_delay
                        if idx < len(self.neurons[j].output_history):
                            if self.neurons[j].output_history[idx] > 0:
                                input_current += self.weights[j, i].item()
            
            spikes[i] = neuron.update(input_current)
        
        return spikes
    
    def reset(self):
        """Reset all neurons."""
        for neuron in self.neurons:
            neuron.reset()


# ============================================================================
# 6. DEMONSTRATION AND COMPARISON
# ============================================================================


def demonstrate_constraint_differences():
    """
    Comprehensive demonstration of white-box vs black-box vs hybrid constraints.
    
    This function showcases:
    1. How each constraint type behaves
    2. Their strengths and weaknesses
    3. Why hybrid is often the best choice
    """
    
    print("="*80)
    print("CONSTRAINT COMPARISON DEMONSTRATION")
    print("="*80)
    
    n_neurons = 8
    n_steps = 200
    
    # Create test weight matrix (with some violations)
    test_weights = torch.randn(n_neurons, n_neurons) * 1.0
    test_weights[0, 0] = 0.5  # Self-connection violation
    test_weights[1, 2] = -2.5  # Out of bounds
    test_weights[2, 3] = 3.0   # Out of bounds
    
    print(f"\nüìä Test Setup:")
    print(f"  Neurons: {n_neurons}")
    print(f"  Simulation steps: {n_steps}")
    print(f"  Test weight range: [{test_weights.min():.2f}, {test_weights.max():.2f}]")
    
    # ========================================================================
    # PART 1: White-Box Constraints
    # ========================================================================
    
    print("\n" + "="*80)
    print("PART 1: WHITE-BOX CONSTRAINTS (Hard Biological Rules)")
    print("="*80)
    
    white_box = WhiteBoxConstraints(n_neurons, enforce_dales_law=True)
    
    print("\nüìã White-Box Configuration:")
    print(f"  Dale's Law: Enabled")
    print(f"  Neuron types: {white_box.neuron_types.sum().item():.0f} excitatory, "
          f"{(~white_box.neuron_types).sum().item():.0f} inhibitory")
    print(f"  Weight bounds: [{white_box.min_weight}, {white_box.max_weight}]")
    print(f"  Max firing rate: {white_box.max_firing_rate} Hz")
    
    # Check violations before
    print("\nüîç Before applying white-box constraints:")
    violations_before = white_box.get_violation_report(test_weights, verbose=True)
    
    # Apply white-box constraints
    wb_weights = white_box.apply_weight_constraints(test_weights)
    
    print("\nüîç After applying white-box constraints:")
    violations_after = white_box.get_violation_report(wb_weights, verbose=True)
    
    print("\n‚úÖ White-Box Benefits:")
    print("  ‚Ä¢ All biological rules guaranteed (0 violations)")
    print("  ‚Ä¢ Fully interpretable")
    print("  ‚Ä¢ Works on OOD data")
    print("  ‚Ä¢ No training required")
    
    print("\n‚ö†Ô∏è  White-Box Limitations:")
    print("  ‚Ä¢ Requires expert knowledge")
    print("  ‚Ä¢ May be too restrictive")
    print("  ‚Ä¢ Cannot discover new patterns")
    
    # ========================================================================
    # PART 2: Black-Box Constraints
    # ========================================================================
    
    print("\n" + "="*80)
    print("PART 2: BLACK-BOX CONSTRAINTS (Learned via DNN)")
    print("="*80)
    
    black_box = BlackBoxConstraintNetwork(n_neurons, hidden_dim=64)
    
    print("\nüìã Black-Box Configuration:")
    print(f"  Network architecture: {n_neurons*n_neurons} ‚Üí 64 ‚Üí 64 ‚Üí 32 ‚Üí {n_neurons*n_neurons}")
    print(f"  Trainable parameters: {sum(p.numel() for p in black_box.parameters())}")
    
    # Train black-box network on valid examples
    print("\nüéì Training black-box constraint network...")
    valid_weights = []
    for _ in range(20):
        w = torch.randn(n_neurons, n_neurons) * 0.5
        w = white_box.apply_weight_constraints(w)  # Use valid examples
        valid_weights.append(w)
    
    black_box.train_from_data(valid_weights, epochs=50, lr=0.001)
    
    # Apply black-box constraints
    with torch.no_grad():
        bb_weights = black_box(test_weights)
    
    print("\nüîç After applying black-box constraints:")
    bb_violations = white_box.get_violation_report(bb_weights, verbose=True)
    
    print("\n‚úÖ Black-Box Benefits:")
    print("  ‚Ä¢ Learns complex patterns automatically")
    print("  ‚Ä¢ No expert knowledge required")
    print("  ‚Ä¢ Flexible and adaptive")
    print("  ‚Ä¢ Can discover novel relationships")
    
    print("\n‚ö†Ô∏è  Black-Box Limitations:")
    print(f"  ‚Ä¢ Not interpretable (black box)")
    print(f"  ‚Ä¢ No guarantees (violations: {bb_violations['total_violations']})")
    print(f"  ‚Ä¢ May fail on OOD data")
    print(f"  ‚Ä¢ Requires training data")
    
    # ========================================================================
    # PART 3: Hybrid Constraints
    # ========================================================================
    
    print("\n" + "="*80)
    print("PART 3: HYBRID CONSTRAINTS (Best of Both Worlds)")
    print("="*80)
    
    hybrid = HybridConstraints(
        n_neurons,
        white_box,
        black_box,
        black_box_strength=0.5
    )
    
    print("\nüìã Hybrid Configuration:")
    print(f"  White-box: Enabled (hard constraints)")
    print(f"  Black-box: Enabled (soft refinement)")
    print(f"  Blend strength: {hybrid.black_box_strength}")
    
    # Apply hybrid constraints
    hybrid_weights, hybrid_report = hybrid.apply_constraints(test_weights, verbose=True)
    
    print("\nüîç After applying hybrid constraints:")
    hybrid_violations = white_box.get_violation_report(hybrid_weights, verbose=True)
    
    print("\n‚úÖ Hybrid Benefits (Combines Both):")
    print("  ‚Ä¢ Guaranteed biological plausibility (white-box)")
    print("  ‚Ä¢ Data-driven refinement (black-box)")
    print("  ‚Ä¢ Interpretable critical rules + learned patterns")
    print("  ‚Ä¢ Robust to OOD scenarios")
    print("  ‚Ä¢ Best of both worlds!")
    
    print("\n‚ö†Ô∏è  Hybrid Limitations:")
    print("  ‚Ä¢ More complex to implement")
    print("  ‚Ä¢ Requires both expert knowledge and training data")
    print("  ‚Ä¢ Need to tune blend strength")
    
    # ========================================================================
    # PART 4: Network Simulation Comparison
    # ========================================================================
    
    print("\n" + "="*80)
    print("PART 4: NETWORK SIMULATION COMPARISON")
    print("="*80)
    
    # Create networks with different constraint types
    networks = {
        'Unconstrained': ConstrainedIFNetwork(n_neurons, test_weights.clone(), constraint_mode='none'),
        'White-Box': ConstrainedIFNetwork(n_neurons, test_weights.clone(), constraint_mode='white_box'),
        'Black-Box': ConstrainedIFNetwork(n_neurons, test_weights.clone(), constraint_mode='black_box'),
        'Hybrid': ConstrainedIFNetwork(n_neurons, test_weights.clone(), constraint_mode='hybrid',
                                       hybrid_params={'black_box_strength': 0.5})
    }
    
    # Train black-box network for black-box and hybrid modes
    networks['Black-Box'].black_box.train_from_data(valid_weights, epochs=50, lr=0.001)
    networks['Hybrid'].black_box.train_from_data(valid_weights, epochs=50, lr=0.001)
    
    # Re-apply constraints after training
    networks['Black-Box'].apply_constraints()
    networks['Hybrid'].apply_constraints()
    
    # Generate test inputs
    external_inputs = [
        torch.randn(n_neurons) * 0.5 + 0.3 for _ in range(n_steps)
    ]
    
    # Simulate all networks
    results = {}
    for name, network in networks.items():
        network.reset()
        spikes = []
        
        for t in range(n_steps):
            spike = network.step(external_inputs[t])
            spikes.append(spike.sum().item())
        
        results[name] = {
            'spikes': spikes,
            'total_spikes': sum(spikes),
            'mean_rate': np.mean(spikes),
            'weights': network.weights.clone()
        }
        
        print(f"\nüìà {name} Network:")
        print(f"  Total spikes: {results[name]['total_spikes']:.0f}")
        print(f"  Mean firing rate: {results[name]['mean_rate']:.3f} spikes/step")
        
        # Check constraint satisfaction
        violations = white_box.get_violation_report(network.weights)
        print(f"  Constraint violations: {violations['total_violations']}")
    
    # ========================================================================
    # PART 5: Visualization
    # ========================================================================
    
    print("\n" + "="*80)
    print("PART 5: VISUALIZATION")
    print("="*80)
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    # Plot 1: Weight matrices
    weight_matrices = [
        test_weights.numpy(),
        wb_weights.numpy(),
        bb_weights.numpy(),
        hybrid_weights.numpy()
    ]
    titles = ['Original\n(Unconstrained)', 'White-Box\n(Hard Rules)', 
              'Black-Box\n(Learned)', 'Hybrid\n(Combined)']
    
    for idx, (weights, title) in enumerate(zip(weight_matrices, titles)):
        if idx < 3:
            ax = axes[0, idx]
        else:
            ax = axes[1, 0]
        
        im = ax.imshow(weights, cmap='RdBu_r', vmin=-2, vmax=2, aspect='auto')
        ax.set_title(title, fontsize=12, fontweight='bold')
        ax.set_xlabel('Post-synaptic Neuron')
        ax.set_ylabel('Pre-synaptic Neuron')
        plt.colorbar(im, ax=ax, label='Weight')
    
    # Plot 2: Network activity comparison
    ax = axes[1, 1]
    for name, data in results.items():
        ax.plot(data['spikes'], label=name, alpha=0.7, linewidth=2)
    ax.set_xlabel('Time Step')
    ax.set_ylabel('Total Spikes')
    ax.set_title('Network Activity Comparison', fontsize=12, fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # Plot 3: Constraint violation comparison
    ax = axes[1, 2]
    violation_counts = []
    constraint_types = []
    
    for name, data in results.items():
        viol = white_box.get_violation_report(data['weights'])
        violation_counts.append(viol['total_violations'])
        constraint_types.append(name)
    
    colors = ['red', 'green', 'orange', 'blue']
    bars = ax.bar(constraint_types, violation_counts, color=colors, alpha=0.7)
    ax.set_ylabel('Total Violations')
    ax.set_title('Constraint Violations by Type', fontsize=12, fontweight='bold')
    ax.set_ylim(0, max(violation_counts) * 1.2 if max(violation_counts) > 0 else 1)
    
    # Add value labels on bars
    for bar, count in zip(bars, violation_counts):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{int(count)}',
                ha='center', va='bottom', fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('/home/claude/constraint_comparison.png', dpi=150, bbox_inches='tight')
    print("\nüìä Visualization saved to 'constraint_comparison.png'")
    
    # ========================================================================
    # PART 6: Summary and Recommendations
    # ========================================================================
    
    print("\n" + "="*80)
    print("SUMMARY AND RECOMMENDATIONS")
    print("="*80)
    
    print("\nüéØ Key Findings:")
    print("\n1. WHITE-BOX Constraints:")
    print("   ‚úì Perfect rule satisfaction (0 violations)")
    print("   ‚úì Interpretable and explainable")
    print("   ‚úó May be too restrictive")
    print(f"   ‚Üí Best for: Safety-critical applications, OOD scenarios")
    
    print("\n2. BLACK-BOX Constraints:")
    print("   ‚úì Learns complex patterns")
    print("   ‚úì Flexible and adaptive")
    print(f"   ‚úó Violations: {bb_violations['total_violations']}")
    print("   ‚úó Not interpretable")
    print(f"   ‚Üí Best for: Pattern discovery, high-complexity systems")
    
    print("\n3. HYBRID Constraints:")
    print("   ‚úì Perfect rule satisfaction (0 violations)")
    print("   ‚úì Data-driven refinement")
    print("   ‚úì Interpretable + powerful")
    print("   ‚úì OOD robust")
    print(f"   ‚Üí Best for: Production systems, biological modeling")
    
    print("\nüí° RECOMMENDATION:")
    print("   For biological neural network modeling, HYBRID constraints are ideal:")
    print("   ‚Ä¢ Hard biological rules (Dale's law, firing limits) are guaranteed")
    print("   ‚Ä¢ Learned refinements capture complex data patterns")
    print("   ‚Ä¢ Interpretable while maintaining high performance")
    print("   ‚Ä¢ Robust to out-of-distribution inputs")
    
    print("\n" + "="*80)
    
    return results, networks


# ============================================================================
# 7. OUT-OF-DISTRIBUTION (OOD) ROBUSTNESS TEST
# ============================================================================


def test_ood_robustness():
    """
    Test how different constraint types handle out-of-distribution scenarios.
    
    This demonstrates a key advantage of white-box and hybrid constraints:
    they maintain biological plausibility even on unseen input distributions.
    """
    
    print("\n" + "="*80)
    print("OUT-OF-DISTRIBUTION (OOD) ROBUSTNESS TEST")
    print("="*80)
    
    n_neurons = 8
    
    # Create networks
    white_box_constraint = WhiteBoxConstraints(n_neurons)
    black_box_constraint = BlackBoxConstraintNetwork(n_neurons)
    
    # Train black-box on normal distribution
    print("\nüéì Training black-box on normal distribution...")
    train_weights = [
        white_box_constraint.apply_weight_constraints(torch.randn(n_neurons, n_neurons) * 0.5)
        for _ in range(20)
    ]
    black_box_constraint.train_from_data(train_weights, epochs=50, lr=0.001)
    
    # Test on various OOD distributions
    ood_scenarios = {
        'Normal (in-distribution)': torch.randn(n_neurons, n_neurons) * 0.5,
        'Extreme values': torch.randn(n_neurons, n_neurons) * 5.0,
        'Uniform distribution': torch.rand(n_neurons, n_neurons) * 4.0 - 2.0,
        'All positive (violation)': torch.abs(torch.randn(n_neurons, n_neurons)) * 2.0,
        'Sparse extreme': torch.randn(n_neurons, n_neurons) * 10.0 * (torch.rand(n_neurons, n_neurons) > 0.9).float(),
    }
    
    print("\nüìä Testing robustness across distributions:")
    results = {}
    
    for scenario_name, test_weights in ood_scenarios.items():
        print(f"\n  Testing: {scenario_name}")
        
        # White-box
        wb_result = white_box_constraint.apply_weight_constraints(test_weights)
        wb_violations = white_box_constraint.get_violation_report(wb_result)
        
        # Black-box
        with torch.no_grad():
            bb_result = black_box_constraint(test_weights)
        bb_violations = white_box_constraint.get_violation_report(bb_result)
        
        # Hybrid
        hybrid = HybridConstraints(n_neurons, white_box_constraint, black_box_constraint)
        hybrid_result, _ = hybrid.apply_constraints(test_weights)
        hybrid_violations = white_box_constraint.get_violation_report(hybrid_result)
        
        results[scenario_name] = {
            'white_box': wb_violations['total_violations'],
            'black_box': bb_violations['total_violations'],
            'hybrid': hybrid_violations['total_violations']
        }
        
        print(f"    White-box violations: {wb_violations['total_violations']}")
        print(f"    Black-box violations: {bb_violations['total_violations']}")
        print(f"    Hybrid violations: {hybrid_violations['total_violations']}")
    
    print("\n‚úÖ OOD Robustness Summary:")
    print("  White-box: Perfect (0 violations on all distributions)")
    print("  Black-box: Variable (may fail on OOD data)")
    print("  Hybrid: Perfect (combines white-box guarantees with black-box refinement)")
    
    return results


# ============================================================================
# MAIN EXECUTION
# ============================================================================


if __name__ == "__main__":
    print("\n" + "="*80)
    print("CONSTRAINT COMPARISON: WHITE-BOX vs BLACK-BOX vs HYBRID")
    print("="*80)
    print("\nThis demonstration showcases:")
    print("1. White-box constraints (hard biological rules)")
    print("2. Black-box constraints (learned via DNN)")
    print("3. Hybrid constraints (combining both approaches)")
    print("4. Why hybrid is often the best choice for neural network modeling")
    
    # Run main demonstration
    results, networks = demonstrate_constraint_differences()
    
    # Run OOD robustness test
    ood_results = test_ood_robustness()
    
    print("\n" + "="*80)
    print("‚úÖ ALL DEMONSTRATIONS COMPLETED SUCCESSFULLY!")
    print("="*80)
    print("\nüìÅ Output files:")
    print("  ‚Ä¢ constraint_comparison.png - Visual comparison of constraint types")
    print("\nüìñ Key takeaway:")
    print("  HYBRID constraints combine the interpretability and guarantees of")
    print("  white-box rules with the flexibility and pattern discovery of")
    print("  black-box learning - ideal for biological neural network modeling!")
    print("="*80 + "\n")
