{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0368139-f63a-459b-8a17-c9d168bdff8b",
   "metadata": {},
   "source": [
    "# RL for Maximizing Q_fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "252e2ce6-689a-4550-a5b0-ac0a910e9724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import gymtorax\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import gymtorax.action_handler as ah\n",
    "import gymtorax.observation_handler as oh\n",
    "from gymtorax.envs.base_env import BaseEnv\n",
    "from gymtorax import rewards as torax_reward\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback\n",
    "from stable_baselines3.common.logger import configure\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import time\n",
    "import logging\n",
    "import warnings\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e628f617-46a5-4b81-a954-cfeff61d77a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger(\"gymtorax\").setLevel(logging.ERROR)\n",
    "logging.getLogger('gymnasium').setLevel(logging.ERROR)\n",
    "logging.getLogger('gym').setLevel(logging.ERROR)\n",
    "logging.getLogger(\"gymtorax.action_handler\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0bd386-b3bb-4abc-b5e2-8c178e7536d2",
   "metadata": {},
   "source": [
    "## Import State Space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d225881-5df6-4364-83ae-4b1ab19c7ade",
   "metadata": {},
   "source": [
    "Our RL agent will only have partial observability into the total state space provided by the TORAX simulator. Here, we import the states we want our agent to be able to observe from a CSV file listing all of the state parameters in the state space. We will then integrate the states that we wish for our agent to observe into our training environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0c40de2-722e-49ca-b070-9551366e3762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OBS_PROFILES = ['T_e', 'T_i', 'n_e', 'psi', 'psi_norm', 'spr', 'R_in', 'R_out', 'delta', 'delta_lower', 'delta_upper', 'elongation', 'magnetic_shear', 'q', 'p_ohmic_e']\n",
      "OBS_SCALARS = ['H20', 'H89P', 'H97L', 'H98', 'Q_fusion', 'beta_N', 'beta_pol', 'beta_tor', 'dW_thermal_dt', 'tau_E', 'E_aux', 'E_fusion', 'P_alpha_total', 'P_aux_total', 'P_cyclotron_e', 'P_SOL_total', 'B_0', 'R_major', 'a_minor', 'q95', 'q_min', 'rho_q_min', 'S_gas_puff', 'S_generic_particle', 'S_pellet', 'S_total', 'W_pol', 'W_thermal_e', 'W_thermal_i', 'W_thermal_total', 'fgw_n_e_line_avg', 'fgw_n_e_volume_avg', 'n_e_line_avg', 'n_e_volume_avg', 'A_i', 'A_impurity', 'I_aux_generic', 'I_bootstrap', 'I_ecrh', 'P_icrh_total', 'Phi_b', 'Phi_b_dot', 'li3', 'v_loop_lcfs']\n"
     ]
    }
   ],
   "source": [
    "# Extract states that are meant to be observable to the RL agent\n",
    "state_space_csv = \"Gym_TORAX_IterHybrid-v0_env - State Space.csv\"\n",
    "df = pd.read_csv(state_space_csv)\n",
    "observable_df = df[df[\"OBSERVABLE?\"] == \"Yes\"][[\"NAME\", \"TYPE\"]].dropna()\n",
    "\n",
    "# Profile (vector) variables\n",
    "OBS_PROFILES = (\n",
    "    observable_df[observable_df[\"TYPE\"] == \"vector\"][\"NAME\"]\n",
    "    .dropna()\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "# Scalar variables\n",
    "OBS_SCALARS = (\n",
    "    observable_df[observable_df[\"TYPE\"] != \"vector\"][\"NAME\"]\n",
    "    .dropna()\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "print(\"OBS_PROFILES =\", OBS_PROFILES)\n",
    "print(\"OBS_SCALARS =\", OBS_SCALARS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3725cad-e0eb-4982-873c-3d4b95b2c192",
   "metadata": {},
   "source": [
    "## Define Training Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016a80ca-4ab0-4d0e-ae67-f28ad8bd20cb",
   "metadata": {},
   "source": [
    "Here we implement the Gymnasium training environment. The environment includes the state space, the observation space (a subset of the state space consisting of the state parameters that the RL agent can observe), the action space, and the reward function. We have modified Gym-TORAX's existing base ITER hybrid environment. Our changes to the existing environment are:\n",
    "\n",
    "1) We have hidden the majority of the state parameters from the RL agent. Only the parameters imported from the CSV file are visible to the agent at any time step.\n",
    "2) We have modified the reward function to suit our custom objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcd52d04-7dd2-408a-aa73-32280d971ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Config for ITER hybrid scenario based parameters with nonlinear solver.\n",
    "\n",
    "ITER hybrid scenario based (roughly) on van Mulders Nucl. Fusion 2021.\n",
    "With Newton-Raphson solver and adaptive timestep (backtracking)\n",
    "\"\"\"\n",
    "\n",
    "_NBI_W_TO_MA = 1 / 16e6  # rough estimate of NBI heating power to current drive\n",
    "W_to_Ne_ratio = 0\n",
    "\n",
    "# No NBI during rampup. Rampup all NBI power between 99-100 seconds\n",
    "nbi_times = np.array([0, 99, 100])\n",
    "nbi_powers = np.array([0, 0, 33e6])\n",
    "nbi_cd = nbi_powers * _NBI_W_TO_MA\n",
    "\n",
    "# Gaussian prescription of \"NBI\" deposition profiles and fractional deposition\n",
    "r_nbi = 0.25\n",
    "w_nbi = 0.25\n",
    "el_heat_fraction = 0.66\n",
    "\n",
    "# No ECCD power for this config (but kept here for future flexibility)\n",
    "eccd_power = {0: 0, 99: 0, 100: 20.0e6}\n",
    "\n",
    "\n",
    "CONFIG = {\n",
    "    \"plasma_composition\": {\n",
    "        \"main_ion\": {\"D\": 0.5, \"T\": 0.5},  # (bundled isotope average)\n",
    "        \"impurity\": {\"Ne\": 1 - W_to_Ne_ratio, \"W\": W_to_Ne_ratio},\n",
    "        \"Z_eff\": {0.0: {0.0: 2.0, 1.0: 2.0}},  # sets impurity densities\n",
    "    },\n",
    "    \"profile_conditions\": {\n",
    "        \"Ip\": {0: 3e6, 100: 12.5e6},  # total plasma current in MA\n",
    "        \"T_i\": {0.0: {0.0: 6.0, 1.0: 0.2}},  # T_i initial condition\n",
    "        \"T_i_right_bc\": 0.2,  # T_i boundary condition\n",
    "        \"T_e\": {0.0: {0.0: 6.0, 1.0: 0.2}},  # T_e initial condition\n",
    "        \"T_e_right_bc\": 0.2,  # T_e boundary condition\n",
    "        \"n_e_right_bc_is_fGW\": True,\n",
    "        \"n_e_right_bc\": {0: 0.35, 100: 0.35},  # n_e boundary condition\n",
    "        # set initial condition density according to Greenwald fraction.\n",
    "        \"nbar\": 0.85,  # line average density for initial condition\n",
    "        \"n_e\": {0: {0.0: 1.3, 1.0: 1.0}},  # Initial electron density profile\n",
    "        \"normalize_n_e_to_nbar\": True,  # normalize initial n_e to nbar\n",
    "        \"n_e_nbar_is_fGW\": True,  # nbar is in units for greenwald fraction\n",
    "        \"initial_psi_from_j\": True,  # initial psi from current formula\n",
    "        \"initial_j_is_total_current\": True,  # only ohmic current on init\n",
    "        \"current_profile_nu\": 2,  # exponent in initial current formula\n",
    "    },\n",
    "    \"numerics\": {\n",
    "        \"t_final\": 150,  # length of simulation time in seconds\n",
    "        \"fixed_dt\": 1,  # fixed timestep\n",
    "        \"evolve_ion_heat\": True,  # solve ion heat equation\n",
    "        \"evolve_electron_heat\": True,  # solve electron heat equation\n",
    "        \"evolve_current\": True,  # solve current equation\n",
    "        \"evolve_density\": True,  # solve density equation\n",
    "    },\n",
    "    \"geometry\": {\n",
    "        \"geometry_type\": \"chease\",\n",
    "        \"geometry_file\": \"ITER_hybrid_citrin_equil_cheasedata.mat2cols\",\n",
    "        \"Ip_from_parameters\": True,\n",
    "        \"R_major\": 6.2,  # major radius (R) in meters\n",
    "        \"a_minor\": 2.0,  # minor radius (a) in meters\n",
    "        \"B_0\": 5.3,  # Toroidal magnetic field on axis [T]\n",
    "    },\n",
    "    \"sources\": {\n",
    "        # Current sources (for psi equation)\n",
    "        \"ecrh\": {  # ECRH/ECCD (with Lin-Liu)\n",
    "            \"gaussian_width\": 0.05,\n",
    "            \"gaussian_location\": 0.35,\n",
    "            \"P_total\": eccd_power,\n",
    "        },\n",
    "        \"generic_heat\": {  # Proxy for NBI heat source\n",
    "            \"gaussian_location\": r_nbi,  # Gaussian location in normalized coordinates\n",
    "            \"gaussian_width\": w_nbi,  # Gaussian width in normalized coordinates\n",
    "            \"P_total\": (nbi_times, nbi_powers),  # Total heating power\n",
    "            # electron heating fraction r\n",
    "            \"electron_heat_fraction\": el_heat_fraction,\n",
    "        },\n",
    "        \"generic_current\": {  # Proxy for NBI current source\n",
    "            \"use_absolute_current\": True,  # I_generic is total external current\n",
    "            \"gaussian_width\": w_nbi,\n",
    "            \"gaussian_location\": r_nbi,\n",
    "            \"I_generic\": (nbi_times, nbi_cd),\n",
    "        },\n",
    "        \"fusion\": {},  # fusion power\n",
    "        \"ei_exchange\": {},  # equipartition\n",
    "        \"ohmic\": {},  # ohmic power\n",
    "        \"cyclotron_radiation\": {},  # cyclotron radiation\n",
    "        \"impurity_radiation\": {  # impurity radiation + bremsstrahlung\n",
    "            \"model_name\": \"mavrin_fit\",\n",
    "            \"radiation_multiplier\": 0.0,\n",
    "        },\n",
    "    },\n",
    "    \"neoclassical\": {\n",
    "        \"bootstrap_current\": {\n",
    "            \"bootstrap_multiplier\": 1.0,\n",
    "        },\n",
    "    },\n",
    "    \"pedestal\": {\n",
    "        \"model_name\": \"set_T_ped_n_ped\",\n",
    "        # use internal boundary condition model (for H-mode and L-mode)\n",
    "        \"set_pedestal\": True,\n",
    "        \"T_i_ped\": {0: 0.5, 100: 0.5, 105: 3.0},\n",
    "        \"T_e_ped\": {0: 0.5, 100: 0.5, 105: 3.0},\n",
    "        \"n_e_ped_is_fGW\": True,\n",
    "        \"n_e_ped\": 0.85,  # pedestal top n_e in units of fGW\n",
    "        \"rho_norm_ped_top\": 0.95,  # set ped top location in normalized radius\n",
    "    },\n",
    "    \"transport\": {\n",
    "        \"model_name\": \"qlknn\",  # Using QLKNN_7_11 default\n",
    "        # set inner core transport coefficients (ad-hoc MHD/EM transport)\n",
    "        \"apply_inner_patch\": True,\n",
    "        \"D_e_inner\": 0.15,\n",
    "        \"V_e_inner\": 0.0,\n",
    "        \"chi_i_inner\": 0.3,\n",
    "        \"chi_e_inner\": 0.3,\n",
    "        \"rho_inner\": 0.1,  # radius below which patch transport is applied\n",
    "        # set outer core transport coefficients (L-mode near edge region)\n",
    "        \"apply_outer_patch\": True,\n",
    "        \"D_e_outer\": 0.1,\n",
    "        \"V_e_outer\": 0.0,\n",
    "        \"chi_i_outer\": 2.0,\n",
    "        \"chi_e_outer\": 2.0,\n",
    "        \"rho_outer\": 0.95,  # radius above which patch transport is applied\n",
    "        # allowed chi and diffusivity bounds\n",
    "        \"chi_min\": 0.05,  # minimum chi\n",
    "        \"chi_max\": 100,  # maximum chi (can be helpful for stability)\n",
    "        \"D_e_min\": 0.05,  # minimum electron diffusivity\n",
    "        \"D_e_max\": 50,  # maximum electron diffusivity\n",
    "        \"V_e_min\": -10,  # minimum electron convection\n",
    "        \"V_e_max\": 10,  # minimum electron convection\n",
    "        \"smoothing_width\": 0.1,\n",
    "        \"DV_effective\": True,\n",
    "        \"include_ITG\": True,  # to toggle ITG modes on or off\n",
    "        \"include_TEM\": True,  # to toggle TEM modes on or off\n",
    "        \"include_ETG\": True,  # to toggle ETG modes on or off\n",
    "        \"avoid_big_negative_s\": False,\n",
    "    },\n",
    "    \"solver\": {\n",
    "        \"solver_type\": \"linear\",  # linear solver with picard iteration\n",
    "        \"use_predictor_corrector\": True,  # for linear solver\n",
    "        \"n_corrector_steps\": 10,  # for linear solver\n",
    "        \"chi_pereverzev\": 30,\n",
    "        \"D_pereverzev\": 15,\n",
    "        \"use_pereverzev\": True,\n",
    "        #        'log_iterations': False,\n",
    "    },\n",
    "    \"time_step_calculator\": {\n",
    "        \"calculator_type\": \"fixed\",\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "class ReducedObservation(oh.Observation):\n",
    "    def __init__(self, profiles=None, scalars=None, custom_bounds_file=None):\n",
    "        variables = {\n",
    "            \"profiles\": profiles if profiles is not None else [],\n",
    "            \"scalars\": scalars if scalars is not None else [],\n",
    "        }\n",
    "\n",
    "        super().__init__(\n",
    "            variables=variables,\n",
    "            custom_bounds_filename=custom_bounds_file\n",
    "        )\n",
    "\n",
    "\n",
    "class IterHybridEnvPartialObservability(BaseEnv):\n",
    "\n",
    "    def __init__(self, render_mode=None, **kwargs):\n",
    "\n",
    "        # Set environment-specific defaults\n",
    "        kwargs.setdefault(\"log_level\", \"warning\")\n",
    "        kwargs.setdefault(\"plot_config\", \"default\")\n",
    "\n",
    "        super().__init__(render_mode=render_mode, **kwargs)\n",
    "\n",
    "    def _define_action_space(self):\n",
    "        actions = [\n",
    "            ah.IpAction(\n",
    "                max=[15e6],  # 15 MA max plasma current\n",
    "                ramp_rate=[0.2e6],\n",
    "            ),  # 0.2 MA/s ramp rate limit\n",
    "            ah.NbiAction(\n",
    "                max=[33e6, 1.0, 1.0],  # 33 MW max NBI power\n",
    "            ),\n",
    "            ah.EcrhAction(\n",
    "                max=[20e6, 1.0, 1.0],  # 20 MW max ECRH power\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        return actions\n",
    "\n",
    "    def _define_observation_space(self):\n",
    "        # return AllObservation(custom_bounds_file=\"gymtorax/envs/iter_hybrid.json\n",
    "        return ReducedObservation(\n",
    "            profiles=OBS_PROFILES,\n",
    "            scalars=OBS_SCALARS,\n",
    "            custom_bounds_file=None\n",
    "        )\n",
    "\n",
    "    def _get_torax_config(self):\n",
    "        return {\n",
    "            \"config\": CONFIG,\n",
    "            \"discretization\": \"fixed\",\n",
    "            \"ratio_a_sim\": 1,\n",
    "        }\n",
    "\n",
    "    def _compute_reward(self, state, next_state, action):\n",
    "        weight_list = [1, 1, 1, 1, 1]\n",
    "\n",
    "        # Rewards are only provided for fusion gain when the plasma is in H-mode\n",
    "        def _is_H_mode():\n",
    "            if (\n",
    "                next_state[\"profiles\"][\"T_e\"][0] > 10\n",
    "                and next_state[\"profiles\"][\"T_i\"][0] > 10\n",
    "            ):\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "        # Fusion gain Q reward: r_fusion = Q / 10\n",
    "        def _r_fusion_gain():\n",
    "            fusion_gain = (torax_reward.get_fusion_gain(next_state) / 10)\n",
    "            if _is_H_mode():\n",
    "                return fusion_gain\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "        # Confinement quality H98 reward: reward H98 value up to 1 (higher H98 value = better confinement)\n",
    "        def _r_h98():\n",
    "            h98 = torax_reward.get_h98(next_state)\n",
    "            if _is_H_mode():\n",
    "                if h98 <= 1:\n",
    "                    return h98\n",
    "                else:\n",
    "                    return 1\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "        # Minimum safety factor reward: reward q_min value up to 1 (q_min < 1 leads to disruptions)\n",
    "        def _r_q_min():\n",
    "            q_min = torax_reward.get_q_min(next_state)\n",
    "            if q_min <= 1:\n",
    "                return q_min\n",
    "            elif q_min > 1:\n",
    "                return 1\n",
    "\n",
    "        # Edge safety factor reward: reward q_95 value / 3 value up to 1\n",
    "        def _r_q_95():\n",
    "            q_95 = torax_reward.get_q95(next_state)\n",
    "            if q_95 / 3 <= 1:\n",
    "                return q_95 / 3\n",
    "            else:\n",
    "                return 1\n",
    "\n",
    "        # Greenwald fraction reward: reward fgw value < 0.9 with 1, 0.9 - 1 with a small reward, > 1 with 0\n",
    "        def _r_greenwald():\n",
    "            fgw = float(next_state[\"scalars\"][\"fgw_n_e_line_avg\"][0])  # Need to custom define this reward\n",
    "        \n",
    "            if fgw <= 0.9:\n",
    "                return 1\n",
    "            elif fgw <= 1:\n",
    "                return 1 - (fgw - 0.9) / 0.1\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "        # Calculate individual reward components\n",
    "        r_fusion_gain = weight_list[0] * _r_fusion_gain() / 50\n",
    "        r_h98 = weight_list[1] * _r_h98() / 50\n",
    "        r_q_min = weight_list[2] * _r_q_min() / 150\n",
    "        r_q_95 = weight_list[3] * _r_q_95() / 150\n",
    "        r_greenwald = weight_list[4] * _r_greenwald() / 150\n",
    "\n",
    "        total_reward = r_fusion_gain + r_h98 + r_q_min + r_q_95 + r_greenwald\n",
    "\n",
    "        return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16e6847-09df-4b22-b083-d0185f5ec30a",
   "metadata": {},
   "source": [
    "## Implement PPO Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cb9076-80f6-4717-ac3a-30d667811a44",
   "metadata": {},
   "source": [
    "Our chosen algorithm is Proximal Policy Optimization, implemented using the Stable-Baselines3 (SB3) library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f0fa29-719a-443d-86f8-9e6cf179f8fa",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "187e9319-df21-46ce-9bd5-7efcc4cd488d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenToraxObservation(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Convert TORAX observation dict:\n",
    "    \n",
    "        {\n",
    "            \"profiles\": {var: np.array(shape=(n_points,))},\n",
    "            \"scalars\":  {var: np.array([value])}\n",
    "        }\n",
    "\n",
    "    into one flat 1D vector Box for use with Stable-Baselines3.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "        obs_space = env.observation_space\n",
    "        assert isinstance(obs_space, spaces.Dict), \\\n",
    "            \"Expected Dict observation space from TORAX.\"\n",
    "\n",
    "        # Store key order so flattening is consistent across time steps\n",
    "        self.profile_keys = list(obs_space[\"profiles\"].spaces.keys())\n",
    "        self.scalar_keys = list(obs_space[\"scalars\"].spaces.keys())\n",
    "\n",
    "        # Compute total dimensionality\n",
    "        dim = 0\n",
    "\n",
    "        # Profiles (vectors)\n",
    "        for key in self.profile_keys:\n",
    "            box = obs_space[\"profiles\"].spaces[key]\n",
    "            dim += int(np.prod(box.shape))\n",
    "\n",
    "        # Scalars\n",
    "        for key in self.scalar_keys:\n",
    "            box = obs_space[\"scalars\"].spaces[key]\n",
    "            dim += int(np.prod(box.shape))\n",
    "\n",
    "        # Define new flattened observation space\n",
    "        low = -np.inf * np.ones(dim, dtype=np.float32)\n",
    "        high = np.inf * np.ones(dim, dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=low, high=high, dtype=np.float32)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        \"\"\"Flatten TORAX nested dict observation into a 1D vector.\"\"\"\n",
    "        parts = []\n",
    "\n",
    "        # Profiles\n",
    "        for key in self.profile_keys:\n",
    "            parts.append(np.asarray(obs[\"profiles\"][key]).ravel())\n",
    "\n",
    "        # Scalars\n",
    "        for key in self.scalar_keys:\n",
    "            parts.append(np.asarray(obs[\"scalars\"][key]).ravel())\n",
    "\n",
    "        flat = np.concatenate(parts).astype(np.float32)\n",
    "        return flat\n",
    "\n",
    "\n",
    "class FlattenToraxAction(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    Convert TORAX action Dict {\n",
    "        'Ip': Box(1,)\n",
    "        'NBI': Box(3,)\n",
    "        'ECRH': Box(3,)\n",
    "    }\n",
    "    into a single 1D Box for SB3, and unflatten on env.step().\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "        act_space = env.action_space\n",
    "        assert isinstance(act_space, spaces.Dict), \\\n",
    "            \"Expected Dict action space from TORAX\"\n",
    "\n",
    "        self.keys = list(act_space.spaces.keys())\n",
    "\n",
    "        # Compute flattened dimension\n",
    "        dims = []\n",
    "        for k in self.keys:\n",
    "            dims.append(int(np.prod(act_space[k].shape)))\n",
    "\n",
    "        self.key_dims = dims\n",
    "        total_dim = sum(dims)\n",
    "\n",
    "        # Create the new flattened Box action space\n",
    "        lows = []\n",
    "        highs = []\n",
    "        for k in self.keys:\n",
    "            box = act_space[k]\n",
    "            lows.append(box.low.flatten())\n",
    "            highs.append(box.high.flatten())\n",
    "\n",
    "        lows = np.concatenate(lows).astype(np.float32)\n",
    "        highs = np.concatenate(highs).astype(np.float32)\n",
    "\n",
    "        self.action_space = spaces.Box(low=lows, high=highs, dtype=np.float32)\n",
    "\n",
    "    def action(self, flat_action):\n",
    "        \"\"\"\n",
    "        Convert flat action vector back into TORAX dict for env.step().\n",
    "        \"\"\"\n",
    "        out = {}\n",
    "        idx = 0\n",
    "        for k, dim in zip(self.keys, self.key_dims):\n",
    "            out[k] = flat_action[idx: idx + dim]\n",
    "            idx += dim\n",
    "        return out\n",
    "\n",
    "\n",
    "class ActionRescaleWrapper(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    Maps actions from [-1, 1] to the TORAX physical action ranges.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "        self.low = env.action_space.low\n",
    "        self.high = env.action_space.high\n",
    "\n",
    "        # New action space presented to the agent\n",
    "        self.action_space = gym.spaces.Box(\n",
    "            low=-1.0, high=1.0, shape=self.low.shape, dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def action(self, normalized_action):\n",
    "        # Convert from [-1,1] to [low, high]\n",
    "        return self.low + (normalized_action + 1.0) * 0.5 * (self.high - self.low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b4ad2e7-0865-4942-98d2-68f671f385a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = IterHybridEnvPartialObservability()\n",
    "# env = FlattenToraxObservation(env)\n",
    "# env = FlattenToraxAction(env)\n",
    "# env = ActionRescaleWrapper(env)\n",
    "\n",
    "# obs, info = env.reset()\n",
    "# print(\"obs shape:\", obs.shape)\n",
    "# print(\"action space:\", env.action_space)\n",
    "# print(\"observation space:\", env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adac3379-1d78-477d-a91b-1157b4490867",
   "metadata": {},
   "source": [
    "### PPO training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02676d48-b0ce-4ee0-8607-c4e27ca585cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment factory\n",
    "def make_env(seed: int | None = None) -> gym.Env:\n",
    "    \"\"\"\n",
    "    Create a single wrapped ITER hybrid environment ready for SB3.\n",
    "    - partial observability\n",
    "    - flattened observations\n",
    "    - flattened actions\n",
    "    \"\"\"\n",
    "    env = IterHybridEnvPartialObservability(render_mode=None, store_history=False)\n",
    "\n",
    "    # Flatten and rescale observation and action dictionaries\n",
    "    env = FlattenToraxObservation(env)\n",
    "    env = FlattenToraxAction(env)\n",
    "    env = ActionRescaleWrapper(env)\n",
    "\n",
    "    if seed is not None:\n",
    "        env.reset(seed=seed)\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24e873ea-dc52-417f-9a3e-dc2efaef1617",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from logs/ppo_iter_2025-11-25_23-05-59/checkpoints/ppo_ckpt_100000_steps.zip\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'logs\\\\ppo_iter_2025-11-25_23-05-59\\\\checkpoints\\\\ppo_ckpt_100000_steps.zip.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resume_checkpoint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     22\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading model from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresume_checkpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     model = \u001b[43mPPO\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_checkpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_env\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m     continue_training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\GT_courses\\CSE_8803-Scientific_Machine_Learning\\Final Project\\RL_Gym_TORAX\\venv\\Lib\\site-packages\\stable_baselines3\\common\\base_class.py:681\u001b[39m, in \u001b[36mBaseAlgorithm.load\u001b[39m\u001b[34m(cls, path, env, device, custom_objects, print_system_info, force_reset, **kwargs)\u001b[39m\n\u001b[32m    678\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m== CURRENT SYSTEM INFO ==\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    679\u001b[39m     get_system_info()\n\u001b[32m--> \u001b[39m\u001b[32m681\u001b[39m data, params, pytorch_variables = \u001b[43mload_from_zip_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_system_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_system_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mNo data found in the saved file\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    689\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mNo params found in the saved file\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\GT_courses\\CSE_8803-Scientific_Machine_Learning\\Final Project\\RL_Gym_TORAX\\venv\\Lib\\site-packages\\stable_baselines3\\common\\save_util.py:403\u001b[39m, in \u001b[36mload_from_zip_file\u001b[39m\u001b[34m(load_path, load_data, custom_objects, device, verbose, print_system_info)\u001b[39m\n\u001b[32m    376\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_from_zip_file\u001b[39m(\n\u001b[32m    377\u001b[39m     load_path: Union[\u001b[38;5;28mstr\u001b[39m, pathlib.Path, io.BufferedIOBase],\n\u001b[32m    378\u001b[39m     load_data: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    382\u001b[39m     print_system_info: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    383\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[Optional[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]], TensorDict, Optional[TensorDict]]:\n\u001b[32m    384\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    385\u001b[39m \u001b[33;03m    Load model data from a .zip archive\u001b[39;00m\n\u001b[32m    386\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    401\u001b[39m \u001b[33;03m        and dict of pytorch variables\u001b[39;00m\n\u001b[32m    402\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m     file = \u001b[43mopen_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mzip\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    405\u001b[39m     \u001b[38;5;66;03m# set device to cpu if cuda is not available\u001b[39;00m\n\u001b[32m    406\u001b[39m     device = get_device(device=device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\functools.py:931\u001b[39m, in \u001b[36msingledispatch.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires at least \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    930\u001b[39m                     \u001b[33m'\u001b[39m\u001b[33m1 positional argument\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m931\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\GT_courses\\CSE_8803-Scientific_Machine_Learning\\Final Project\\RL_Gym_TORAX\\venv\\Lib\\site-packages\\stable_baselines3\\common\\save_util.py:240\u001b[39m, in \u001b[36mopen_path_str\u001b[39m\u001b[34m(path, mode, verbose, suffix)\u001b[39m\n\u001b[32m    225\u001b[39m \u001b[38;5;129m@open_path\u001b[39m.register(\u001b[38;5;28mstr\u001b[39m)\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mopen_path_str\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m, mode: \u001b[38;5;28mstr\u001b[39m, verbose: \u001b[38;5;28mint\u001b[39m = \u001b[32m0\u001b[39m, suffix: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m) -> io.BufferedIOBase:\n\u001b[32m    227\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    228\u001b[39m \u001b[33;03m    Open a path given by a string. If writing to the path, the function ensures\u001b[39;00m\n\u001b[32m    229\u001b[39m \u001b[33;03m    that the path exists.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    238\u001b[39m \u001b[33;03m    :return:\u001b[39;00m\n\u001b[32m    239\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopen_path_pathlib\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpathlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\GT_courses\\CSE_8803-Scientific_Machine_Learning\\Final Project\\RL_Gym_TORAX\\venv\\Lib\\site-packages\\stable_baselines3\\common\\save_util.py:291\u001b[39m, in \u001b[36mopen_path_pathlib\u001b[39m\u001b[34m(path, mode, verbose, suffix)\u001b[39m\n\u001b[32m    285\u001b[39m         path.parent.mkdir(exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m, parents=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    287\u001b[39m \u001b[38;5;66;03m# if opening was successful uses the open_path() function\u001b[39;00m\n\u001b[32m    288\u001b[39m \u001b[38;5;66;03m# if opening failed with IsADirectory|FileNotFound, calls open_path_pathlib\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;66;03m#   with corrections\u001b[39;00m\n\u001b[32m    290\u001b[39m \u001b[38;5;66;03m# if reading failed with FileNotFoundError, calls open_path_pathlib with suffix\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopen_path_pathlib\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\GT_courses\\CSE_8803-Scientific_Machine_Learning\\Final Project\\RL_Gym_TORAX\\venv\\Lib\\site-packages\\stable_baselines3\\common\\save_util.py:272\u001b[39m, in \u001b[36mopen_path_pathlib\u001b[39m\u001b[34m(path, mode, verbose, suffix)\u001b[39m\n\u001b[32m    270\u001b[39m             path, suffix = newpath, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    271\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Desktop\\GT_courses\\CSE_8803-Scientific_Machine_Learning\\Final Project\\RL_Gym_TORAX\\venv\\Lib\\site-packages\\stable_baselines3\\common\\save_util.py:264\u001b[39m, in \u001b[36mopen_path_pathlib\u001b[39m\u001b[34m(path, mode, verbose, suffix)\u001b[39m\n\u001b[32m    262\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    263\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m open_path(\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m, mode, verbose, suffix)\n\u001b[32m    265\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[32m    266\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m suffix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m suffix != \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\pathlib\\_local.py:537\u001b[39m, in \u001b[36mPath.open\u001b[39m\u001b[34m(self, mode, buffering, encoding, errors, newline)\u001b[39m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m    536\u001b[39m     encoding = io.text_encoding(encoding)\n\u001b[32m--> \u001b[39m\u001b[32m537\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'logs\\\\ppo_iter_2025-11-25_23-05-59\\\\checkpoints\\\\ppo_ckpt_100000_steps.zip.zip'"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "# Make training and eval environment\n",
    "train_env = make_env(seed=0)\n",
    "eval_env = make_env(seed=1)\n",
    "\n",
    "# Logging + checkpoints\n",
    "run_name = datetime.now().strftime(\"ppo_iter_%Y-%m-%d_%H-%M-%S\")\n",
    "log_dir = os.path.join(\"logs\", run_name)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "checkpoint_dir = os.path.join(log_dir, \"checkpoints\")\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "tensorboard_log = os.path.join(log_dir, \"tb\")\n",
    "\n",
    "# Resume training from checkpoint\n",
    "# resume_checkpoint = None\n",
    "resume_checkpoint = \"logs/ppo_iter_2025-11-25_23-05-59/checkpoints/ppo_ckpt_100000_steps.zip\"\n",
    "\n",
    "if resume_checkpoint is not None:\n",
    "    print(f\"Loading model from {resume_checkpoint}\")\n",
    "    model = PPO.load(resume_checkpoint, env=train_env)\n",
    "    continue_training = True\n",
    "else:\n",
    "    print(\"Starting NEW PPO training.\")\n",
    "    continue_training = False\n",
    "\n",
    "# PPO moded trained from scratch\n",
    "if not continue_training:\n",
    "    model = PPO(\n",
    "        policy=\"MlpPolicy\",\n",
    "        env=train_env,\n",
    "        verbose=1,\n",
    "        n_steps=2048,\n",
    "        batch_size=64,\n",
    "        gamma=0.99,\n",
    "        learning_rate=3e-4,\n",
    "        ent_coef=0.0,\n",
    "        clip_range=0.2,\n",
    "        gae_lambda=0.95,\n",
    "        n_epochs=10,\n",
    "        tensorboard_log=tensorboard_log,\n",
    "        policy_kwargs=dict(net_arch=[256, 256]),\n",
    "    )\n",
    "\n",
    "# Resuming training\n",
    "else:\n",
    "    model.set_env(train_env)\n",
    "    model.tensorboard_log = tensorboard_log\n",
    "\n",
    "# Configure SB3 logger\n",
    "new_logger = configure(log_dir, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "model.set_logger(new_logger)\n",
    "\n",
    "# Save a checkpoint every N steps\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=5000,  # env steps\n",
    "    save_path=checkpoint_dir,\n",
    "    name_prefix=\"ppo_ckpt\",\n",
    ")\n",
    "\n",
    "# Evaluate periodically on a separate environment\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=os.path.join(log_dir, \"best_model\"),\n",
    "    log_path=os.path.join(log_dir, \"eval\"),\n",
    "    eval_freq=25_000,  # How often to run eval (steps)\n",
    "    n_eval_episodes=3,\n",
    "    deterministic=True,\n",
    ")\n",
    "\n",
    "# Total timesteps\n",
    "total_timesteps = 200_000\n",
    "\n",
    "model.learn(\n",
    "    total_timesteps=total_timesteps,\n",
    "    callback=[checkpoint_callback, eval_callback],\n",
    "    progress_bar=True,\n",
    "    reset_num_timesteps=not continue_training,\n",
    ")\n",
    "\n",
    "# Save final model\n",
    "model.save(os.path.join(log_dir, \"ppo_iter_final\"))\n",
    "\n",
    "train_env.close()\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b32e05-1587-4e4b-8cc9-2c358c2354a9",
   "metadata": {},
   "source": [
    "## Evaluate Policy Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec565f71-0770-4bca-91b5-f43a785bf4ae",
   "metadata": {},
   "source": [
    "Evaluate the trained RL agent. Our policy is deterministically chosen for evaluations to always choose the mean action values given a trained policy--this empirically leads to better evaluation performance. Unfortunately, the TORAX simulator is deterministic, leading to the same evaluation output when the actions are deterministically chosen as well. Training RL agents robust to noise in the simulated data would be a good line of future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b19d468f-3eae-4865-af8e-82ba333ac005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_physics_metrics(state):\n",
    "    \"\"\"Extract key physics values from the TORAX state dict.\"\"\"\n",
    "    return {\n",
    "        \"q_min\": torax_reward.get_q_min(state),\n",
    "        \"q_95\": torax_reward.get_q95(state),\n",
    "        \"H98\": torax_reward.get_h98(state),\n",
    "        \"fusion_Q\": torax_reward.get_fusion_gain(state),\n",
    "        \"fGW\": float(state[\"scalars\"][\"fgw_n_e_line_avg\"][0]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08975f77-a554-45d1-b9b2-72184e04bdd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PPO model from: logs/ppo_iter_2025-11-26_09-21-05/best_model/best_model.zip\n",
      "Episode 1/5 — Reward = 29.339 \n",
      "Mean inference time per step: 1.719 ms\n",
      "Std  inference time per step: 0.442 ms\n",
      "Min  inference time: 0.620 ms\n",
      "Max  inference time: 3.530 ms\n",
      "Saved episode trajectory to: evaluation_results/episode_1_trajectory.csv\n",
      "\n",
      "Episode 2/5 — Reward = 29.339 \n",
      "Mean inference time per step: 0.846 ms\n",
      "Std  inference time per step: 0.107 ms\n",
      "Min  inference time: 0.668 ms\n",
      "Max  inference time: 1.476 ms\n",
      "Saved episode trajectory to: evaluation_results/episode_2_trajectory.csv\n",
      "\n",
      "Episode 3/5 — Reward = 29.339 \n",
      "Mean inference time per step: 0.855 ms\n",
      "Std  inference time per step: 0.082 ms\n",
      "Min  inference time: 0.698 ms\n",
      "Max  inference time: 1.300 ms\n",
      "Saved episode trajectory to: evaluation_results/episode_3_trajectory.csv\n",
      "\n",
      "Episode 4/5 — Reward = 29.339 \n",
      "Mean inference time per step: 0.868 ms\n",
      "Std  inference time per step: 0.094 ms\n",
      "Min  inference time: 0.705 ms\n",
      "Max  inference time: 1.411 ms\n",
      "Saved episode trajectory to: evaluation_results/episode_4_trajectory.csv\n",
      "\n",
      "Episode 5/5 — Reward = 29.339 \n",
      "Mean inference time per step: 0.873 ms\n",
      "Std  inference time per step: 0.121 ms\n",
      "Min  inference time: 0.684 ms\n",
      "Max  inference time: 1.757 ms\n",
      "Saved episode trajectory to: evaluation_results/episode_5_trajectory.csv\n",
      "\n",
      "\n",
      "======== Evaluation Summary ========\n",
      "Evaluated 5 episodes\n",
      "Mean episode reward: 29.339 ± 0.000\n",
      "Results saved to: evaluation_results\n",
      "====================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"logs/ppo_iter_2025-11-26_09-21-05/best_model/best_model.zip\"\n",
    "NUM_EPISODES = 5\n",
    "OUTPUT_DIR = \"evaluation_results\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Loading PPO model from: {MODEL_PATH}\")\n",
    "model = PPO.load(MODEL_PATH)\n",
    "\n",
    "# Create evaluation environment\n",
    "eval_env = DummyVecEnv([lambda: make_env()])\n",
    "\n",
    "all_episode_rewards = []\n",
    "all_metrics = []\n",
    "\n",
    "for ep in range(NUM_EPISODES):  \n",
    "    seed = np.random.randint(0, 999)\n",
    "    eval_env.seed(seed)\n",
    "    obs = eval_env.reset()\n",
    "    ep_reward = 0.0\n",
    "    truncated = False\n",
    "\n",
    "    traj_actions = []\n",
    "    traj_rewards = []\n",
    "    traj_metrics = []\n",
    "    inference_times = []   # store per-step times\n",
    "\n",
    "    while True:\n",
    "\n",
    "        # Time inference\n",
    "        t0 = time.perf_counter()\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        t1 = time.perf_counter()\n",
    "        inference_times.append(t1 - t0)\n",
    "        \n",
    "        next_obs, reward, done, info = eval_env.step(action)\n",
    "\n",
    "        r = float(reward[0])\n",
    "        d = bool(done[0])\n",
    "        truncated = info[0].get(\"TimeLimit.truncated\", False)\n",
    "\n",
    "        # Get underlying TORAX state for metrics\n",
    "        true_env = eval_env.envs[0].unwrapped\n",
    "        state = true_env.state\n",
    "        metrics = compute_physics_metrics(state)\n",
    "\n",
    "        # Log\n",
    "        ep_reward += r\n",
    "        traj_actions.append(action[0])\n",
    "        traj_rewards.append(r)\n",
    "        traj_metrics.append(metrics)\n",
    "\n",
    "        # Prepare next step\n",
    "        obs = next_obs\n",
    "\n",
    "        if d:  # terminated OR truncated\n",
    "            break\n",
    "\n",
    "    all_episode_rewards.append(ep_reward)\n",
    "    all_metrics.append(traj_metrics)\n",
    "\n",
    "    # Print episode reward\n",
    "    print(f\"Episode {ep+1}/{NUM_EPISODES} — Reward = {ep_reward:.3f} \"\n",
    "          f\"{'(truncated)' if truncated else ''}\")\n",
    "\n",
    "    # Print inference time\n",
    "    print(f\"Mean inference time per step: {np.mean(inference_times)*1000:.3f} ms\")\n",
    "    print(f\"Std  inference time per step: {np.std(inference_times)*1000:.3f} ms\")\n",
    "    print(f\"Min  inference time: {np.min(inference_times)*1000:.3f} ms\")\n",
    "    print(f\"Max  inference time: {np.max(inference_times)*1000:.3f} ms\")\n",
    "\n",
    "    # Save trajectory as CSV\n",
    "    df = pd.DataFrame(traj_metrics)\n",
    "    df[\"reward\"] = traj_rewards\n",
    "    csv_path = f\"{OUTPUT_DIR}/episode_{ep+1}_trajectory.csv\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved episode trajectory to: {csv_path}\")\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "    # Prepare data for plotting\n",
    "    actions = np.array(traj_actions)\n",
    "    qmin_series = [m[\"q_min\"]    for m in traj_metrics]\n",
    "    q95_series  = [m[\"q_95\"]     for m in traj_metrics]\n",
    "    fgw_series  = [m[\"fGW\"]      for m in traj_metrics]\n",
    "    Q_series    = [m[\"fusion_Q\"] for m in traj_metrics]\n",
    "    H98_series  = [m[\"H98\"]      for m in traj_metrics]\n",
    "\n",
    "    # Actions\n",
    "    plt.figure(figsize=(10, 4), dpi=120)\n",
    "    plt.title(f\"Episode {ep+1}: Actions\")\n",
    "    plt.plot(actions[:, 0], label=\"Ip command\")\n",
    "    plt.plot(actions[:, 1], label=\"NBI power\")\n",
    "    plt.plot(actions[:, 2], label=\"ECRH power\")\n",
    "    plt.xlabel(\"Timestep\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/episode_{ep+1}_actions.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # q_min\n",
    "    plt.figure(figsize=(10, 4), dpi=120)\n",
    "    plt.title(f\"Episode {ep+1}: q_min(t)\")\n",
    "    plt.plot(qmin_series)\n",
    "    plt.axhline(1.0, color=\"r\", linestyle=\"--\", label=\"q_min = 1 limit\")\n",
    "    plt.xlabel(\"Timestep\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/episode_{ep+1}_qmin.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # q_95\n",
    "    plt.figure(figsize=(10, 4), dpi=120)\n",
    "    plt.title(f\"Episode {ep+1}: q_95(t)\")\n",
    "    plt.plot(q95_series)\n",
    "    plt.axhline(3.0, color=\"r\", linestyle=\"--\", label=\"q_95 = 3 limit\")\n",
    "    plt.xlabel(\"Timestep\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/episode_{ep+1}_q95.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # fGW\n",
    "    plt.figure(figsize=(10, 4), dpi=120)\n",
    "    plt.title(f\"Episode {ep+1}: Greenwald Fraction fGW(t)\")\n",
    "    plt.plot(fgw_series)\n",
    "    plt.axhline(0.9, color=\"r\", linestyle=\"--\", label=\"fGW = 0.9 limit\")\n",
    "    plt.xlabel(\"Timestep\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/episode_{ep+1}_fgw.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Fusion gain Q\n",
    "    plt.figure(figsize=(10, 4), dpi=120)\n",
    "    plt.title(f\"Episode {ep+1}: Fusion Gain Q(t)\")\n",
    "    plt.plot(Q_series)\n",
    "    plt.xlabel(\"Timestep\")\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/episode_{ep+1}_fusionQ.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # H98\n",
    "    plt.figure(figsize=(10, 4), dpi=120)\n",
    "    plt.title(f\"Episode {ep+1}: H98(t)\")\n",
    "    plt.plot(H98_series)\n",
    "    plt.xlabel(\"Timestep\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/episode_{ep+1}_h98.png\")\n",
    "    plt.close()\n",
    "\n",
    "# Overall summary\n",
    "mean_reward = np.mean(all_episode_rewards)\n",
    "std_reward  = np.std(all_episode_rewards)\n",
    "\n",
    "print(\"\\n======== Evaluation Summary ========\")\n",
    "print(f\"Evaluated {NUM_EPISODES} episodes\")\n",
    "print(f\"Mean episode reward: {mean_reward:.3f} ± {std_reward:.3f}\")\n",
    "print(f\"Results saved to: {OUTPUT_DIR}\")\n",
    "print(\"====================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948cd0a8-65ed-481f-bd1e-b88fb91b067c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
